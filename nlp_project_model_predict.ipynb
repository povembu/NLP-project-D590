{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOucjnC1j4om9yDQxZPknDB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/povembu/NLP-project-D590/blob/main/nlp_project_model_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hate Speech Text Classification"
      ],
      "metadata": {
        "id": "iokKxBQofa7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Packages"
      ],
      "metadata": {
        "id": "tFZi8C-5ffDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import unicodedata\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import nltk\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8--XrmddfaNP",
        "outputId": "13c556a4-a0a1-44be-8c30-a44d623a1d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare NLP Functions"
      ],
      "metadata": {
        "id": "Erlg_4WZfow9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words.remove('no')\n",
        "stop_words.remove('but')\n",
        "stop_words.remove('not')"
      ],
      "metadata": {
        "id": "RVX4NBGjclSy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "tokenizer = ToktokTokenizer()"
      ],
      "metadata": {
        "id": "jHB4uxw7clVM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_porter_stemming(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=stop_words):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "def normalize_text(corpus, text_lower_case=True,\n",
        "                     text_stemming=False, text_lemmatization=True,\n",
        "                     special_char_removal=True, remove_digits=True,\n",
        "                     stopword_removal=True,stopwords=stop_words):\n",
        "\n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "\n",
        "        # remove extra newlines\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "          doc = lemmatize_text(doc)\n",
        "\n",
        "        # stem text\n",
        "        if text_stemming and not text_lemmatization:\n",
        "          doc = simple_porter_stemming(doc)\n",
        "\n",
        "        # remove special characters and\\or digits\n",
        "        if special_char_removal:\n",
        "          # insert spaces between special characters to isolate them\n",
        "          special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "          doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "          doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "\n",
        "         # lowercase the text\n",
        "        if text_lower_case:\n",
        "          doc = doc.lower()\n",
        "\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "          doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = doc.strip()\n",
        "\n",
        "        normalized_corpus.append(doc)\n",
        "\n",
        "    return normalized_corpus"
      ],
      "metadata": {
        "id": "SFC_aiqycqop"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and Training Data"
      ],
      "metadata": {
        "id": "yLnlBB6yftHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMtQr1VgfTgC",
        "outputId": "782263ca-e8aa-4517-8d96-9164ed22909a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load processed training data\n",
        "with open('/content/drive/My Drive/Colab Notebooks/NLP Class/Final Project/train_data.pkl','rb') as f:\n",
        "    norm_train_X = pickle.load(f)"
      ],
      "metadata": {
        "id": "GrxBjhTXeXJO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build BOW features on train text\n",
        "cv = CountVectorizer(ngram_range=(1,2),stop_words=stop_words).fit(norm_train_X)"
      ],
      "metadata": {
        "id": "PfH-1ndLc4D8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved logistic regression model\n",
        "with open('/content/drive/My Drive/Colab Notebooks/NLP Class/Final Project/lr_model.pkl','rb') as f:\n",
        "    lr_cv_load = pickle.load(f)"
      ],
      "metadata": {
        "id": "aYg_60Tec9z1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify Text"
      ],
      "metadata": {
        "id": "nndDeiy7fw1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grab text sample from user imput\n",
        "#text_sample = user.input()"
      ],
      "metadata": {
        "id": "-LS2s9KzhJuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize input text and convert features into vectors\n",
        "text_sample=[\"new foreigners , who reckon they can live amongest superiour , pagan blood\"]\n",
        "\n",
        "norm_text = normalize_text(text_sample)\n",
        "\n",
        "cv_text = cv.transform(norm_text)\n",
        "scores = lr_cv_load.predict_proba(cv_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE7KFVs9dQdg",
        "outputId": "41900ce8-a227-4499-c050-409ea270a7f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_sample)\n",
        "if scores[:,1] > 0.5:\n",
        "  print(\"This comment contains hate\")\n",
        "else:\n",
        "  print(\"Not a hate comment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dskwy3jsdU4p",
        "outputId": "3edac767-07ab-4f02-db89-de9a3c1ab5cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['new foreigners , who reckon they can live amongest superiour , pagan blood']\n",
            "This comment contains hate\n"
          ]
        }
      ]
    }
  ]
}